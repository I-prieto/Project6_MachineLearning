{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hxmivoc14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Impact Analysis\n",
    "def calculate_business_impact(y_true, y_pred):\n",
    "    service_rank = {'Normal': 1, 'Urgent': 2, 'Critical': 3}\n",
    "    tier_cost = 100  # $ per tier\n",
    "    late_risk = 500  # $ late delivery cost\n",
    "    \n",
    "    overspend = sum((service_rank[pred] - service_rank[true]) * tier_cost \n",
    "                   for true, pred in zip(y_true, y_pred) if service_rank[pred] > service_rank[true])\n",
    "    \n",
    "    late_cost = sum(late_risk for true, pred in zip(y_true, y_pred) \n",
    "                   if service_rank[pred] < service_rank[true])\n",
    "    \n",
    "    correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)\n",
    "    \n",
    "    return {'overspend': overspend, 'late_cost': late_cost, 'correct': correct,\n",
    "            'total_cost': overspend + late_cost}\n",
    "\n",
    "baseline_impact = calculate_business_impact(y_test, y_pred_baseline)\n",
    "best_impact = calculate_business_impact(y_test, best_predictions)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BUSINESS IMPACT (Test Set)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBaseline:\")\n",
    "print(f\"  Correct: {baseline_impact['correct']}/{len(y_test)} ({baseline_impact['correct']/len(y_test)*100:.1f}%)\")\n",
    "print(f\"  Total cost: ${baseline_impact['total_cost']:,}\")\n",
    "\n",
    "print(f\"\\nBest Model:\")\n",
    "print(f\"  Correct: {best_impact['correct']}/{len(y_test)} ({best_impact['correct']/len(y_test)*100:.1f}%)\")\n",
    "print(f\"  Total cost: ${best_impact['total_cost']:,}\")\n",
    "\n",
    "print(f\"\\nðŸ’° Savings: ${baseline_impact['total_cost'] - best_impact['total_cost']:,}\")\n",
    "print(f\"ðŸ“Š Improvement: {(1 - best_impact['total_cost']/baseline_impact['total_cost'])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rl7uwvb17z",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance (for tree-based models)\n",
    "best_classifier = rf_pipeline.named_steps['classifier']  # or gb_pipeline\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "feature_names = (numerical_features + \n",
    "                list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)))\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': best_classifier.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 20\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_20 = importance_df.head(20)\n",
    "plt.barh(range(len(top_20)), top_20['importance'])\n",
    "plt.yticks(range(len(top_20)), top_20['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 20 Most Important Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 Features:\")\n",
    "print(importance_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dyup7kwdqb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Best Model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Use best model predictions (adjust based on results)\n",
    "best_predictions = y_pred_rf  # Change to y_pred_gb if Gradient Boosting is best\n",
    "\n",
    "cm = confusion_matrix(y_test, best_predictions, labels=['Normal', 'Urgent', 'Critical'])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Normal', 'Urgent', 'Critical'],\n",
    "            yticklabels=['Normal', 'Urgent', 'Critical'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Best Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Correct predictions: {cm.diagonal().sum()}/{len(y_test)} ({cm.diagonal().sum()/len(y_test)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ek20rtd6v44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison Summary\n",
    "import pandas as pd\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Baseline', 'Logistic Regression', 'Random Forest', 'Gradient Boosting'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, y_pred_baseline),\n",
    "        accuracy_score(y_test, y_pred_lr),\n",
    "        accuracy_score(y_test, y_pred_rf),\n",
    "        accuracy_score(y_test, y_pred_gb)\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_score(y_test, y_pred_baseline, average='weighted'),\n",
    "        f1_score(y_test, y_pred_lr, average='weighted'),\n",
    "        f1_score(y_test, y_pred_rf, average='weighted'),\n",
    "        f1_score(y_test, y_pred_gb, average='weighted')\n",
    "    ]\n",
    "}).sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(comparison.to_string(index=False))\n",
    "print(f\"\\nðŸ† Best Model: {comparison.iloc[0]['Model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qtqtec5auva",
   "metadata": {},
   "source": [
    "### Model Comparison & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h4fmmr96peo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=5,\n",
    "                                             subsample=0.8, random_state=42))\n",
    "])\n",
    "\n",
    "print(\"Training Gradient Boosting...\")\n",
    "gb_pipeline.fit(X_train, y_train)\n",
    "y_pred_gb = gb_pipeline.predict(X_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GRADIENT BOOSTING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_gb):.3f}\")\n",
    "print(f\"F1-Score (weighted): {f1_score(y_test, y_pred_gb, average='weighted'):.3f}\")\n",
    "print(f\"\\n{classification_report(y_test, y_pred_gb)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93zknjv2e2l",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=200, max_depth=20, min_samples_split=5,\n",
    "                                         class_weight='balanced', random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "print(\"Training Random Forest...\")\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "y_pred_rf = rf_pipeline.predict(X_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RANDOM FOREST\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.3f}\")\n",
    "print(f\"F1-Score (weighted): {f1_score(y_test, y_pred_rf, average='weighted'):.3f}\")\n",
    "print(f\"\\n{classification_report(y_test, y_pred_rf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pxa0qa0c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(multi_class='multinomial', solver='lbfgs', \n",
    "                                     class_weight='balanced', max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "y_pred_lr = lr_pipeline.predict(X_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.3f}\")\n",
    "print(f\"F1-Score (weighted): {f1_score(y_test, y_pred_lr, average='weighted'):.3f}\")\n",
    "print(f\"\\n{classification_report(y_test, y_pred_lr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ooo2gqiuq5m",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Baseline (DummyClassifier)\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "baseline_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DummyClassifier(strategy='most_frequent', random_state=42))\n",
    "])\n",
    "\n",
    "baseline_pipeline.fit(X_train, y_train)\n",
    "y_pred_baseline = baseline_pipeline.predict(X_test)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE MODEL (Always predict most frequent)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_baseline):.3f}\")\n",
    "print(f\"F1-Score (weighted): {f1_score(y_test, y_pred_baseline, average='weighted'):.3f}\")\n",
    "print(f\"\\n{classification_report(y_test, y_pred_baseline)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hl8tb6l3fsb",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qdvdf40ygqd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Build Preprocessing Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"âœ“ Preprocessing pipeline created\")\n",
    "print(f\"  - Numerical features (StandardScaler): {numerical_features}\")\n",
    "print(f\"  - Categorical features (OneHotEncoder): {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2n9dleozkfl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train-Test Split (stratified by ZONE)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=df_analysis['ZONE'], random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test:  {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nTarget distribution in train:\\n{y_train.value_counts()}\")\n",
    "print(f\"\\nTarget distribution in test:\\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0hvg6uwgwmdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Prepare X (features) and y (target) - NO DATA LEAKAGE\n",
    "# Only use information known at BOOKING TIME\n",
    "\n",
    "categorical_features = ['ZONE', 'Direction', 'Type', 'DGR', 'iso_country_origin', 'iso_country_destination']\n",
    "numerical_features = ['Gwgt', 'Cwgt', 'Pcs', 'day_of_week', 'month', 'hour']\n",
    "all_features = categorical_features + numerical_features\n",
    "\n",
    "X = df_analysis[all_features].copy()\n",
    "y = df_analysis['optimal_service_level'].copy()\n",
    "\n",
    "print(f\"âœ“ Feature matrix: {X.shape}\")\n",
    "print(f\"âœ“ Target: {y.shape}\")\n",
    "print(f\"\\nFeatures ({len(all_features)}): {all_features}\")\n",
    "print(\"\\nðŸ”’ DATA LEAKAGE CHECK: leadtime_ml_hours NOT in features (correct!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y4dm5e2suc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Feature Engineering - Extract temporal features\n",
    "df_analysis['day_of_week'] = df_analysis['NOTIFICATION date & Time'].dt.dayofweek  # 0=Monday\n",
    "df_analysis['month'] = df_analysis['NOTIFICATION date & Time'].dt.month\n",
    "df_analysis['hour'] = df_analysis['NOTIFICATION date & Time'].dt.hour\n",
    "\n",
    "print(\"âœ“ Temporal features created from notification timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ittkvco9ii",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create optimal service level labels (vectorized, using leadtime_ml_hours)\n",
    "import numpy as np\n",
    "\n",
    "# Create SLA lookup columns (vectorized)\n",
    "df_analysis['sla_normal'] = df_analysis['ZONE'].map(lambda z: sla_targets_lookup.get((z, 'Normal'), float('inf')))\n",
    "df_analysis['sla_urgent'] = df_analysis['ZONE'].map(lambda z: sla_targets_lookup.get((z, 'Urgent'), float('inf')))\n",
    "df_analysis['sla_critical'] = df_analysis['ZONE'].map(lambda z: sla_targets_lookup.get((z, 'Critical'), float('inf')))\n",
    "\n",
    "# Vectorized label creation using leadtime_ml_hours (no outliers)\n",
    "conditions = [\n",
    "    df_analysis['leadtime_ml_hours'] <= df_analysis['sla_normal'],\n",
    "    df_analysis['leadtime_ml_hours'] <= df_analysis['sla_urgent'],\n",
    "    df_analysis['leadtime_ml_hours'] <= df_analysis['sla_critical']\n",
    "]\n",
    "choices = ['Normal', 'Urgent', 'Critical']\n",
    "df_analysis['optimal_service_level'] = np.select(conditions, choices, default='Normal')\n",
    "\n",
    "# Clean up\n",
    "df_analysis.drop(['sla_normal', 'sla_urgent', 'sla_critical'], axis=1, inplace=True)\n",
    "\n",
    "print(\"âœ“ Optimal service level labels created (vectorized, 10-100x faster)\")\n",
    "print(f\"\\nDistribution:\")\n",
    "print(df_analysis['optimal_service_level'].value_counts())\n",
    "print(f\"\\nPercentages:\")\n",
    "print((df_analysis['optimal_service_level'].value_counts(normalize=True) * 100).round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q4nqqvr84sn",
   "metadata": {},
   "source": [
    "## MACHINE LEARNING - Optimal Service Level Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb47bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4901e579",
   "metadata": {},
   "source": [
    "## DATA LOADING, CLEANING & INTEGRATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "\n",
    "orders = pd.read_csv(\"Orders.csv\", encoding=\"latin-1\", sep=\";\", decimal =\",\")\n",
    "leadtime = pd.read_csv(\"LeadtimeService.csv\", encoding=\"latin-1\", sep=\";\")\n",
    "airports = pd.read_csv(\"airports.csv\", encoding=\"latin-1\", sep=\",\", decimal =\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0575c0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2296be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns for analysis from orders dataset\n",
    "\n",
    "cols_keep = [\n",
    "    \"Type\",\n",
    "    \"Direction\",\n",
    "    \"DSV-CW Ref.\",\n",
    "    \"Goods Description\",\n",
    "    \"Pcs\",\n",
    "    \"Gwgt\",\n",
    "    \"Cwgt\",\n",
    "    \"NOTIFICATION date & Time\",\n",
    "    \"ACTUAL Delivery & Time\",\n",
    "    \"Service Level\",\n",
    "    \"DGR\",\n",
    "    \"Real Origin Airport\",\n",
    "    \"Real Destination Airport\",\n",
    "    \"ZONE\"\n",
    "]\n",
    "\n",
    "df_analysis = orders[cols_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c34f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis[\"Service Level\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows with service level: \"ERROR en Service Level (columna AK)\"\n",
    "\n",
    "df_analysis = df_analysis[df_analysis[\"Service Level\"] != \"ERROR en Service Level (columna AK)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e28ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis[\"Service Level\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc9f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service Level mapping used in the analysis\n",
    "service_level_mapping = {\n",
    "    \"ROV\": \"Normal\",\n",
    "    \"CRV\": \"Urgent\",\n",
    "    \"AOV\": \"Critical\"\n",
    "}\n",
    "\n",
    "df_analysis[\"Service Type\"] = (\n",
    "    df_analysis[\"Service Level\"]\n",
    "    .map(service_level_mapping)\n",
    ")\n",
    "\n",
    "df_analysis[[\"Service Level\", \"Service Type\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9946823",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis[\"ZONE\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e17771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "leadtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3325bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop spaces in column names\n",
    "df_analysis.columns = df_analysis.columns.str.strip()\n",
    "leadtime.columns = leadtime.columns.str.strip()\n",
    "\n",
    "# Clean string columns to ensure proper merging\n",
    "df_analysis[\"ZONE\"] = df_analysis[\"ZONE\"].astype(str).str.strip()\n",
    "df_analysis[\"Service Level\"] = df_analysis[\"Service Level\"].astype(str).str.strip()\n",
    "\n",
    "leadtime[\"Zone\"] = leadtime[\"Zone\"].astype(str).str.strip()\n",
    "leadtime[\"SLA\"] = leadtime[\"SLA\"].astype(str).str.strip()\n",
    "\n",
    "# Merge lead time information into the main dataframe\n",
    "\n",
    "df_analysis = df_analysis.merge(\n",
    "    leadtime[[\"Zone\", \"SLA\", \"Leadtime\"]],\n",
    "    how=\"left\",\n",
    "    left_on=[\"ZONE\", \"Service Level\"],\n",
    "    right_on=[\"Zone\", \"SLA\"]\n",
    ")\n",
    "\n",
    "# Limpiar columnas auxiliares del join\n",
    "df_analysis.drop(columns=[\"Zone\", \"SLA\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bf9f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846ab386",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab09af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop spaces in column names\n",
    "airports.columns = airports.columns.str.strip()\n",
    "\n",
    "# Clean string columns to ensure proper merging\n",
    "df_analysis[\"Real Origin Airport\"] = df_analysis[\"Real Origin Airport\"].astype(str).str.strip()\n",
    "df_analysis[\"Real Destination Airport\"] = df_analysis[\"Real Destination Airport\"].astype(str).str.strip()\n",
    "\n",
    "airports[\"iata_code\"] = airports[\"iata_code\"].astype(str).str.strip()\n",
    "\n",
    "# Merge information into the main dataframe for ORIGIN airport\n",
    "df_analysis = df_analysis.merge(\n",
    "    airports[[\"iata_code\", \"iso_country\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"Real Origin Airport\",\n",
    "    right_on=\"iata_code\"\n",
    ")\n",
    "\n",
    "# Rename columns to avoid confusion\n",
    "df_analysis.rename(\n",
    "    columns={\n",
    "        \"iata_code\": \"iata_code_origin\",\n",
    "        \"iso_country\": \"iso_country_origin\"\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# Merge information into the main dataframe for DESTINATION airport\n",
    "df_analysis = df_analysis.merge(\n",
    "    airports[[\"iata_code\", \"iso_country\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"Real Destination Airport\",\n",
    "    right_on=\"iata_code\"\n",
    ")\n",
    "\n",
    "# Rename columns to avoid confusion\n",
    "df_analysis.rename(\n",
    "    columns={\n",
    "        \"iata_code\": \"iata_code_destination\",\n",
    "        \"iso_country\": \"iso_country_destination\"\n",
    "    },\n",
    "    inplace=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7fe18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataframe for analysis\n",
    "df_analysis = df_analysis.drop(columns=[\"Real Origin Airport\", \"Real Destination Airport\"])\n",
    "df_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff7a039",
   "metadata": {},
   "source": [
    "## TARGET ENGINEERING (leadtime_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a6b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert date columns to datetime format\n",
    "\n",
    "notification_col = \"NOTIFICATION date & Time\"\n",
    "delivery_col = \"ACTUAL Delivery & Time\"\n",
    "\n",
    "df_analysis[notification_col] = pd.to_datetime(\n",
    "    df_analysis[notification_col],\n",
    "    errors=\"coerce\",\n",
    "    dayfirst=True\n",
    ")\n",
    "\n",
    "df_analysis[delivery_col] = pd.to_datetime(\n",
    "    df_analysis[delivery_col],\n",
    "    errors=\"coerce\",\n",
    "    dayfirst=True\n",
    ")\n",
    "\n",
    "print(\"\\nMissing timestamps after datetime conversion:\")\n",
    "display(df_analysis[[notification_col, delivery_col]].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows with missing timestamps\n",
    "df_analysis = df_analysis.dropna(subset=[notification_col, delivery_col])\n",
    "\n",
    "df_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d4a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating leadtime real in hours\n",
    "\n",
    "df_analysis[\"leadtime_real_hours\"] = (\n",
    "    (df_analysis[delivery_col] - df_analysis[notification_col])\n",
    "    .dt.total_seconds() / 3600\n",
    ")\n",
    "\n",
    "df_analysis[[\"leadtime_real_hours\", notification_col, delivery_col]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd284dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative lead times check because of data errors\n",
    "print(\"Negatives:\", (df_analysis[\"leadtime_real_hours\"] < 0).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09308f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with negative and 0 lead times\n",
    "\n",
    "negatives_df = df_analysis[df_analysis[\"leadtime_real_hours\"] <= 0]\n",
    "\n",
    "print(f\"Negative lead times: {len(negatives_df)}\")\n",
    "\n",
    "display(\n",
    "    negatives_df[\n",
    "        [\n",
    "            \"NOTIFICATION date & Time\",\n",
    "            \"ACTUAL Delivery & Time\",\n",
    "            \"leadtime_real_hours\",\n",
    "            \"ZONE\",\n",
    "            \"Service Level\",\n",
    "            \"Service Type\"\n",
    "        ]\n",
    "    ].sort_values(\"leadtime_real_hours\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23063d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flagging invalid lead times\n",
    "\n",
    "df_analysis[\"invalid_leadtime_flag\"] = (df_analysis[\"leadtime_real_hours\"] <= 0).astype(int)\n",
    "\n",
    "df_analysis[\"invalid_leadtime_flag\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed958915",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis[df_analysis[\"leadtime_real_hours\"] > 0].copy()\n",
    "\n",
    "#Statistics summary\n",
    "print(df_analysis[\"leadtime_real_hours\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c86c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a target with 99th percentile lead time for avoid outliers\n",
    "\n",
    "leadtime_99th_percentile = df_analysis[\"leadtime_real_hours\"].quantile(0.99)\n",
    "\n",
    "df_analysis[\"leadtime_ml_hours\"] = df_analysis[\"leadtime_real_hours\"].clip(upper=leadtime_99th_percentile)\n",
    "\n",
    "#Statistics summary\n",
    "print(df_analysis[\"leadtime_ml_hours\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a264815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leadtime_real_hours: real operational lead time (EDA)\n",
    "# leadtime_ml_hours: capped target used for ML training with 99th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cfe69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating on_time comparing leadtime_real_hours with SLA\n",
    "df_analysis[\"on_time\"] = (df_analysis[\"leadtime_real_hours\"] <= df_analysis[\"Leadtime\"]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e83b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256cc605",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1077de67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ranking SLA service levels (1=cheapest, 3=most expensive)\n",
    "sla_rank = {\n",
    "    \"Normal\": 1,\n",
    "    \"Urgent\": 2,\n",
    "    \"Critical\": 3\n",
    "}\n",
    "\n",
    "# Create a lookup dictionary for SLA targets by ZONE + Service Type\n",
    "sla_targets_lookup = (\n",
    "    df_analysis\n",
    "    .groupby([\"ZONE\", \"Service Type\"])[\"Leadtime\"]\n",
    "    .first()\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Function to determine minimum required SLA based on actual delivery time and zone\n",
    "def calculate_minimum_required_sla(row):\n",
    "    \"\"\"\n",
    "    Determines the cheapest SLA level that would have met the delivery time\n",
    "    for the specific ZONE of this order.\n",
    "    \"\"\"\n",
    "    actual_leadtime = row[\"leadtime_real_hours\"]\n",
    "    zone = row[\"ZONE\"]\n",
    "    \n",
    "    # Check from cheapest to most expensive for this specific zone\n",
    "    service_levels = [\"Normal\", \"Urgent\", \"Critical\"]\n",
    "    \n",
    "    for service_type in service_levels:\n",
    "        target_key = (zone, service_type)\n",
    "        if target_key in sla_targets_lookup:\n",
    "            target_hours = sla_targets_lookup[target_key]\n",
    "            if actual_leadtime <= target_hours:\n",
    "                return service_type\n",
    "    \n",
    "    return \"Out of SLA\"\n",
    "\n",
    "# Apply the function\n",
    "df_analysis[\"sla_required\"] = df_analysis.apply(calculate_minimum_required_sla, axis=1)\n",
    "\n",
    "# Recommend SLA based on delivery performance\n",
    "def recommend_sla(row):\n",
    "    if row[\"sla_required\"] == \"Out of SLA\":\n",
    "        return \"Normal\"  # Late delivery gets minimum rate\n",
    "    elif row[\"sla_required\"] != row[\"Service Type\"]:\n",
    "        return row[\"sla_required\"]  # Could have used cheaper SLA\n",
    "    else:\n",
    "        return row[\"Service Type\"]  # Keep contracted level\n",
    "\n",
    "df_analysis[\"sla_recommended\"] = df_analysis.apply(recommend_sla, axis=1)\n",
    "\n",
    "# Map to ranks for comparison\n",
    "df_analysis[\"sla_contracted_rank\"] = df_analysis[\"Service Type\"].map(sla_rank)\n",
    "df_analysis[\"sla_recommended_rank\"] = df_analysis[\"sla_recommended\"].map(sla_rank)\n",
    "\n",
    "# Calculate levels to downgrade (how many ranks cheaper)\n",
    "df_analysis[\"levels_to_downgrade\"] = (\n",
    "    df_analysis[\"sla_contracted_rank\"] - df_analysis[\"sla_recommended_rank\"]\n",
    ")\n",
    "\n",
    "# Downgrade is only possible if there are actual savings (levels_to_downgrade > 0)\n",
    "df_analysis[\"downgrade_possible\"] = (df_analysis[\"levels_to_downgrade\"] > 0)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DOWNGRADE ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal orders analyzed: {len(df_analysis)}\")\n",
    "print(f\"Orders with downgrade opportunity: {df_analysis['downgrade_possible'].sum()}\")\n",
    "print(f\"  - With savings (levels > 0): {(df_analysis['levels_to_downgrade'] > 0).sum()}\")\n",
    "print(f\"  - No savings (levels = 0): {(df_analysis['levels_to_downgrade'] == 0).sum()}\")\n",
    "print(f\"Downgrade rate: {df_analysis['downgrade_possible'].mean():.1%}\")\n",
    "\n",
    "print(\"\\nBreakdown by levels_to_downgrade:\")\n",
    "print(df_analysis['levels_to_downgrade'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nDowngrade opportunities by contracted Service Type:\")\n",
    "downgrade_by_service = df_analysis.groupby(\"Service Type\").agg({\n",
    "    'downgrade_possible': ['sum', 'mean'],\n",
    "    'levels_to_downgrade': 'mean'\n",
    "})\n",
    "print(downgrade_by_service)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe498a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXAMPLES:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. Downgrade possible (levels > 0):\")\n",
    "print(df_analysis[df_analysis[\"downgrade_possible\"]][\n",
    "    [\"DSV-CW Ref.\", \"ZONE\", \"Service Type\", \"leadtime_real_hours\", \"Leadtime\", \n",
    "     \"sla_recommended\", \"levels_to_downgrade\", \"downgrade_possible\"]\n",
    "].head(3))\n",
    "\n",
    "print(\"\\n2. No downgrade (levels = 0, already at minimum or met SLA):\")\n",
    "print(df_analysis[df_analysis[\"levels_to_downgrade\"] == 0][\n",
    "    [\"DSV-CW Ref.\", \"ZONE\", \"Service Type\", \"leadtime_real_hours\", \"Leadtime\", \n",
    "     \"sla_recommended\", \"levels_to_downgrade\", \"downgrade_possible\"]\n",
    "].head(3))\n",
    "\n",
    "print(\"\\n3. Specific order SBCN0260531:\")\n",
    "specific_order = df_analysis[df_analysis[\"DSV-CW Ref.\"] == \"SBCN0260531\"][\n",
    "    [\"DSV-CW Ref.\", \"ZONE\", \"Service Type\", \"leadtime_real_hours\", \"Leadtime\", \n",
    "     \"sla_recommended\", \"levels_to_downgrade\", \"downgrade_possible\"]\n",
    "]\n",
    "if len(specific_order) > 0:\n",
    "    print(specific_order)\n",
    "else:\n",
    "    print(\"Order not found in filtered dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4aa0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd2ad34",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16f5f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sla_target(zone, service_type):\n",
    "    \"\"\"\n",
    "    Return SLA (hours) for a given zone and service_type.\n",
    "    Tries:\n",
    "      1) zone + service_type lookup (sla_targets_lookup)\n",
    "      2) type-level default (sla_targets_by_type)\n",
    "      3) final fallback: +inf (so comparisons fail safely)\n",
    "    \"\"\"\n",
    "    key = (zone, service_type)\n",
    "    if \"sla_targets_lookup\" in globals() and key in sla_targets_lookup:\n",
    "        try:\n",
    "            return float(sla_targets_lookup[key])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if \"sla_targets_by_type\" in globals() and service_type in sla_targets_by_type:\n",
    "        try:\n",
    "            return float(sla_targets_by_type[service_type])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return float(\"inf\")\n",
    "\n",
    "\n",
    "def create_optimal_service_label(row):\n",
    "    \"\"\"\n",
    "    For each historical shipment, determine the MINIMUM service level\n",
    "    that would have delivered within SLA.\n",
    "\n",
    "    This becomes our ground truth: what we SHOULD have booked.\n",
    "    \"\"\"\n",
    "    actual_delivery_time = row[\"leadtime_real_hours\"]\n",
    "    zone = row[\"ZONE\"]\n",
    "\n",
    "    # Get SLA targets for this zone (always numeric thanks to get_sla_target)\n",
    "    normal_sla = get_sla_target(zone, \"Normal\")\n",
    "    urgent_sla = get_sla_target(zone, \"Urgent\")\n",
    "    critical_sla = get_sla_target(zone, \"Critical\")\n",
    "\n",
    "    # Assign minimum sufficient service level (check from cheapest to most expensive)\n",
    "    if actual_delivery_time <= normal_sla:\n",
    "        return \"Normal\"  # Cheapest option would have worked\n",
    "    elif actual_delivery_time <= urgent_sla:\n",
    "        return \"Urgent\"  # Needed mid-tier\n",
    "    elif actual_delivery_time <= critical_sla:\n",
    "        return \"Critical\"  # Needed premium\n",
    "    else:\n",
    "        return \"Normal\"  # Even Critical wouldn't have worked â†’ default to cheapest\n",
    "\n",
    "\n",
    "# Apply to dataset\n",
    "df_analysis[\"optimal_service_level\"] = df_analysis.apply(\n",
    "    create_optimal_service_label, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4216561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of optimal service levels\n",
    "print(\"=\"*60)\n",
    "print(\"OPTIMAL SERVICE LEVEL DISTRIBUTION (What we SHOULD have booked)\")\n",
    "print(\"=\"*60)\n",
    "print(df_analysis[\"optimal_service_level\"].value_counts())\n",
    "print(f\"\\nPercentages:\")\n",
    "print(df_analysis[\"optimal_service_level\"].value_counts(normalize=True).round(3) * 100)\n",
    "\n",
    "# Compare with what was actually booked\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: Actual vs. Optimal\")\n",
    "print(\"=\"*60)\n",
    "comparison = pd.crosstab(\n",
    "    df_analysis[\"Service Type\"],  # What was booked\n",
    "    df_analysis[\"optimal_service_level\"],  # What should have been booked\n",
    "    margins=True\n",
    ")\n",
    "print(comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f6196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract temporal features from notification timestamp\n",
    "df_analysis[\"day_of_week\"] = df_analysis[\"NOTIFICATION date & Time\"].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "df_analysis[\"month\"] = df_analysis[\"NOTIFICATION date & Time\"].dt.month\n",
    "df_analysis[\"hour\"] = df_analysis[\"NOTIFICATION date & Time\"].dt.hour\n",
    "\n",
    "print(\"\\nTemporal features created:\")\n",
    "print(df_analysis[[\"NOTIFICATION date & Time\", \"day_of_week\", \"month\", \"hour\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1fe902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "categorical_features = [\"ZONE\", \"Direction\", \"Type\", \"DGR\", \"iso_country_origin\", \"iso_country_destination\"]\n",
    "numerical_features = [\"Gwgt\", \"Cwgt\", \"Pcs\", \"day_of_week\", \"month\", \"hour\"]\n",
    "\n",
    "# Create X (features) and y (target)\n",
    "X = df_analysis[categorical_features + numerical_features].copy()\n",
    "y = df_analysis[\"optimal_service_level\"].copy()\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeatures: {X.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24ed7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split with stratification by ZONE (ensure all zones in both sets)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=df_analysis[\"ZONE\"],  # Maintain zone distribution\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set:  {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nTarget distribution in train set:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nTarget distribution in test set:\")\n",
    "print(y_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5869122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSING PIPELINE CREATED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Numerical features (will be standardized): {numerical_features}\")\n",
    "print(f\"Categorical features (will be one-hot encoded): {categorical_features}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e213d16",
   "metadata": {},
   "source": [
    "## Train Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f795166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Create pipeline with DummyClassifier\n",
    "baseline_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DummyClassifier(strategy='most_frequent', random_state=42))\n",
    "])\n",
    "\n",
    "# Train\n",
    "baseline_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_baseline = baseline_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE MODEL (DummyClassifier - Most Frequent)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_baseline):.3f}\")\n",
    "print(f\"Weighted F1-Score: {f1_score(y_test, y_pred_baseline, average='weighted'):.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_baseline))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_baseline))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10763e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create pipeline\n",
    "lr_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        multi_class='multinomial',\n",
    "        solver='lbfgs',\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining Logistic Regression...\")\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_lr = lr_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.3f}\")\n",
    "print(f\"Weighted F1-Score: {f1_score(y_test, y_pred_lr, average='weighted'):.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46596117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        min_samples_split=5,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining Random Forest...\")\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_rf = rf_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RANDOM FOREST\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.3f}\")\n",
    "print(f\"Weighted F1-Score: {f1_score(y_test, y_pred_rf, average='weighted'):.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d13c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Create pipeline\n",
    "gb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining Gradient Boosting...\")\n",
    "gb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_gb = gb_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GRADIENT BOOSTING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_gb):.3f}\")\n",
    "print(f\"Weighted F1-Score: {f1_score(y_test, y_pred_gb, average='weighted'):.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_gb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98abb7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comparison dataframe\n",
    "models_comparison = pd.DataFrame({\n",
    "    'Model': ['Baseline (Dummy)', 'Logistic Regression', 'Random Forest', 'Gradient Boosting'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, y_pred_baseline),\n",
    "        accuracy_score(y_test, y_pred_lr),\n",
    "        accuracy_score(y_test, y_pred_rf),\n",
    "        accuracy_score(y_test, y_pred_gb)\n",
    "    ],\n",
    "    'Weighted F1-Score': [\n",
    "        f1_score(y_test, y_pred_baseline, average='weighted'),\n",
    "        f1_score(y_test, y_pred_lr, average='weighted'),\n",
    "        f1_score(y_test, y_pred_rf, average='weighted'),\n",
    "        f1_score(y_test, y_pred_gb, average='weighted')\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Sort by F1-Score\n",
    "models_comparison = models_comparison.sort_values('Weighted F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(models_comparison.to_string(index=False))\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = models_comparison.iloc[0]['Model']\n",
    "print(f\"\\nðŸ† Best Model: {best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f1815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Determine which model is best (use the actual best model predictions)\n",
    "# For this example, let's use Random Forest (update based on your results)\n",
    "best_predictions = y_pred_rf  # Change this to y_pred_gb if Gradient Boosting is best\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, best_predictions, labels=['Normal', 'Urgent', 'Critical'])\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Normal', 'Urgent', 'Critical'],\n",
    "            yticklabels=['Normal', 'Urgent', 'Critical'],\n",
    "            ax=ax)\n",
    "ax.set_xlabel('Predicted Service Level')\n",
    "ax.set_ylabel('Actual Optimal Service Level')\n",
    "ax.set_title('Confusion Matrix - Best Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze errors\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nDiagonal (correct predictions):\", cm.diagonal().sum())\n",
    "print(\"Below diagonal (overestimated - predicted higher than needed):\", sum([cm[i,j] for i in range(3) for j in range(3) if i < j]))\n",
    "print(\"Above diagonal (underestimated - predicted lower than needed):\", sum([cm[i,j] for i in range(3) for j in range(3) if i > j]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ab1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance from Random Forest or Gradient Boosting\n",
    "# Get the classifier from the pipeline\n",
    "best_classifier = rf_pipeline.named_steps['classifier']  # or gb_pipeline if GB is best\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "feature_names = (\n",
    "    numerical_features + \n",
    "    list(preprocessor.named_transformers_['cat']\n",
    "         .get_feature_names_out(categorical_features))\n",
    ")\n",
    "\n",
    "# Get importance\n",
    "feature_importance = best_classifier.feature_importances_\n",
    "\n",
    "# Create DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 20\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_20 = importance_df.head(20)\n",
    "plt.barh(range(len(top_20)), top_20['importance'])\n",
    "plt.yticks(range(len(top_20)), top_20['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 20 Most Important Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP 10 MOST IMPORTANT FEATURES\")\n",
    "print(\"=\"*60)\n",
    "print(importance_df.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d860c048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate business metrics\n",
    "def calculate_business_impact(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate cost impact of predictions.\n",
    "    \"\"\"\n",
    "    service_rank = {'Normal': 1, 'Urgent': 2, 'Critical': 3}\n",
    "    tier_cost = 100  # $ per service tier difference\n",
    "    late_delivery_cost = 500  # $ opportunity cost if predict too low\n",
    "    \n",
    "    total_overspend = 0\n",
    "    total_late_risk = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        true_rank = service_rank[true]\n",
    "        pred_rank = service_rank[pred]\n",
    "        \n",
    "        if pred_rank == true_rank:\n",
    "            correct_predictions += 1\n",
    "        elif pred_rank > true_rank:  # Overestimated\n",
    "            total_overspend += (pred_rank - true_rank) * tier_cost\n",
    "        elif pred_rank < true_rank:  # Underestimated\n",
    "            total_late_risk += late_delivery_cost\n",
    "    \n",
    "    return {\n",
    "        'correct': correct_predictions,\n",
    "        'total_overspend': total_overspend,\n",
    "        'total_late_risk': total_late_risk,\n",
    "        'net_cost': total_overspend + total_late_risk\n",
    "    }\n",
    "\n",
    "# Calculate for baseline vs best model\n",
    "baseline_impact = calculate_business_impact(y_test, y_pred_baseline)\n",
    "best_impact = calculate_business_impact(y_test, best_predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BUSINESS IMPACT ANALYSIS (Test Set)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBaseline Model (Always predict most frequent):\")\n",
    "print(f\"  Correct predictions: {baseline_impact['correct']}/{len(y_test)} ({baseline_impact['correct']/len(y_test)*100:.1f}%)\")\n",
    "print(f\"  Total overspend: ${baseline_impact['total_overspend']:,}\")\n",
    "print(f\"  Total late risk: ${baseline_impact['total_late_risk']:,}\")\n",
    "print(f\"  Net cost: ${baseline_impact['net_cost']:,}\")\n",
    "\n",
    "print(f\"\\nBest ML Model:\")\n",
    "print(f\"  Correct predictions: {best_impact['correct']}/{len(y_test)} ({best_impact['correct']/len(y_test)*100:.1f}%)\")\n",
    "print(f\"  Total overspend: ${best_impact['total_overspend']:,}\")\n",
    "print(f\"  Total late risk: ${best_impact['total_late_risk']:,}\")\n",
    "print(f\"  Net cost: ${best_impact['net_cost']:,}\")\n",
    "\n",
    "print(f\"\\nðŸ’° Cost Reduction: ${baseline_impact['net_cost'] - best_impact['net_cost']:,}\")\n",
    "print(f\"ðŸ“Š Improvement: {(1 - best_impact['net_cost']/baseline_impact['net_cost'])*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486dfc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the best model\n",
    "best_model = rf_pipeline  # or gb_pipeline, depending on results\n",
    "\n",
    "joblib.dump(best_model, 'best_service_level_model.pkl')\n",
    "print(\"\\nâœ… Best model saved as 'best_service_level_model.pkl'\")\n",
    "\n",
    "# To load later:\n",
    "# loaded_model = joblib.load('best_service_level_model.pkl')\n",
    "# predictions = loaded_model.predict(new_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
